{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import sys\n",
    "import import_ipynb\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacy import displacy\n",
    "from spacy.symbols import nsubj, dobj, pobj, NOUN, VERB, aux\n",
    "import pkg_resources\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from pathlib import Path\n",
    "\n",
    "# %%\n",
    "#from neo4jQueries import QueryGraph\n",
    "from new_queries import QueryGraph\n",
    "\n",
    "# %%\n",
    "class Converter:\n",
    "    def getVerbsAndNouns(doc):\n",
    "        verbs = []\n",
    "        nouns = []\n",
    "        for token in doc:\n",
    "            if token.pos == VERB and token.dep != aux and token.text != \"want\":\n",
    "                verbs.append(token)\n",
    "            elif token.pos == NOUN:\n",
    "                nouns.append(token)\n",
    "        return verbs, nouns\n",
    "\n",
    "\n",
    "    def getObject(token):\n",
    "        if token.dep == pobj or token.dep == dobj:\n",
    "            return token\n",
    "        else:\n",
    "            for child in token.children:\n",
    "                obj = Converter.getObject(child)\n",
    "                if(obj):\n",
    "                    return obj\n",
    "        return None\n",
    "\n",
    "\n",
    "    def getObjectWithChunk(token, doc):\n",
    "        obj = Converter.getObject(token)\n",
    "        if not obj:\n",
    "            return None\n",
    "        else:\n",
    "            obj_ = {}\n",
    "            obj_[\"text\"] = obj.text\n",
    "            for chunk in doc.noun_chunks:\n",
    "                if obj.text != chunk.text and obj.text in chunk.text:\n",
    "                    obj_[\"chunk\"] = chunk.text\n",
    "                    break\n",
    "            return obj_\n",
    "\n",
    "\n",
    "    def tokenizeTagWords(text):\n",
    "        global nlp\n",
    "        doc = nlp(text)\n",
    "        res = {}\n",
    "        try:\n",
    "            verbs, nouns = Converter.getVerbsAndNouns(doc)\n",
    "            main_verb = verbs[0]\n",
    "\n",
    "            # Add verb\n",
    "            res[\"main_verb\"] = main_verb.text\n",
    "\n",
    "            # Create and add main object\n",
    "            obj_ = Converter.getObjectWithChunk(main_verb, doc)\n",
    "            if obj_ is None and len(nouns) > 0:\n",
    "                res[\"main_object\"] = {\"text\": nouns[0].text}\n",
    "            else:\n",
    "                res[\"main_object\"] = obj_\n",
    "\n",
    "            num_verbs = len(verbs)\n",
    "            if num_verbs > 1 and \"and\" in text:\n",
    "                # There might be another action\n",
    "                sec_verbs = []\n",
    "                for i in range(1, num_verbs):\n",
    "                    sec_verbs.append(\n",
    "                        {\"verb\": verbs[i].text, \"object\": Converter.getObjectWithChunk(verbs[i], doc)})\n",
    "                res[\"sec-verbs\"] = sec_verbs\n",
    "        except Exception as error:\n",
    "            #print(error.args)\n",
    "            pass\n",
    "        finally:\n",
    "            return res\n",
    "\n",
    "\n",
    "    def lowerFirstWord(sentence):\n",
    "        words = sentence.split(' ')\n",
    "        words[0] = words[0].lower()\n",
    "        return \" \".join(words)\n",
    "\n",
    "\n",
    "    def printStories(file):\n",
    "        user_stories = nlp(file)\n",
    "        sentences = [sent.string.strip() for sent in user_stories.sents]\n",
    "        parsed_stories = []\n",
    "        # As a user, I want to click on the address, so that it takes me to a new tab with Google Maps.\n",
    "\n",
    "        for story in sentences:\n",
    "            story = story.strip()\n",
    "            if not story.endswith('.'):\n",
    "                story += '.'\n",
    "            actor = re.search(r'(A|a)s\\s*?an?\\s*([^,]*)?,', story)\n",
    "            action = re.search(\n",
    "                r'[I|i]\\s+?(want to|can|would like to)\\s*([^,\\.]*?)\\s*(,|\\.|so that)', story)\n",
    "            benefit = re.search(r'so that\\s*(.*)\\.$', story)\n",
    "\n",
    "            parsed_story = {}\n",
    "            if actor:\n",
    "                parsed_story[\"actor\"] = actor.group(2)\n",
    "            if action:\n",
    "                parsed_story[\"action\"] = action.group(2)\n",
    "            if benefit:\n",
    "                parsed_story[\"benefit\"] = benefit.group(1)\n",
    "\n",
    "            if parsed_story == {}:\n",
    "                continue\n",
    "\n",
    "            parsed_story[\"tokens\"] = {}\n",
    "\n",
    "            element = \"action\"\n",
    "\n",
    "            if element in parsed_story:\n",
    "                action = \"I want to \" + Converter.lowerFirstWord(parsed_story[element])\n",
    "                parsed_story[\"tokens\"][element] = Converter.tokenizeTagWords(action)\n",
    "\n",
    "            element = \"benefit\"\n",
    "\n",
    "            if element in parsed_story:\n",
    "                benefit = Converter.lowerFirstWord(parsed_story[element])\n",
    "                parsed_story[\"tokens\"][element] = Converter.tokenizeTagWords(benefit)\n",
    "\n",
    "            parsed_stories.append(parsed_story) \n",
    "        return parsed_stories\n",
    "\n",
    "\n",
    "    def getFeatures(json_dict):\n",
    "        actors, main_verb, main_obj, benefit_verb, benefit_obj = [], [], [], [], []\n",
    "        for item in json_dict:\n",
    "            #spell_corr(item.get('actor'))\n",
    "            actors.append(item.get('actor'))\n",
    "            #spell_corr(item.get('tokens').get('action').get('main_verb'))\n",
    "            main_verb.append(item.get('tokens').get('action').get('main_verb'))\n",
    "\n",
    "            if len(item.get('tokens').get('action'))>0:\n",
    "                #spell_corr(item.get('tokens').get('action').get('main_object').get('chunk'))\n",
    "                main_obj.append(item.get('tokens').get('action').get('main_object').get('chunk'))\n",
    "            else:\n",
    "                main_obj.append(None)    \n",
    "\n",
    "            if (item.get('tokens').get('benefit')) is not None:\n",
    "                benefit_verb.append(item.get('tokens').get('benefit').get('main_verb'))            \n",
    "            else:\n",
    "                benefit_verb.append('None') \n",
    "\n",
    "            if item.get('tokens').get('benefit') is not None and (item.get('tokens').get('benefit').get('main_object')) is not None and len(item.get('tokens').get('benefit').get('main_object'))>0:\n",
    "                if item.get('tokens').get('benefit').get('main_object').get('chunk') is not None:\n",
    "                    benefit_obj.append(item.get('tokens').get('benefit').get('main_object').get('chunk')) \n",
    "                else:\n",
    "                    benefit_obj.append(item.get('tokens').get('benefit').get('main_object').get('text'))\n",
    "            else:\n",
    "                benefit_obj.append('None')             \n",
    "\n",
    "        return actors, main_verb, main_obj, benefit_verb, benefit_obj\n",
    "    \n",
    "    \n",
    "    def create_csv(actorsCol, mainVerbsCol, mainObjCol, benefitVerbCol, benefitObjCol):\n",
    "        dataset = pd.DataFrame({'actors': list(actorsCol), 'mainVerbs': list(mainVerbsCol), 'mainObjects' : list(mainObjCol), 'benefitVerbs': list(benefitVerbCol), 'benefitObjects' : list(benefitObjCol) }, columns=['actors', 'mainVerbs', 'mainObjects', 'benefitVerbs', 'benefitObjects'])\n",
    "        cleanedSet = dataset.dropna()\n",
    "        cleanedSet.drop_duplicates(keep='first',inplace=True) \n",
    "        cleanedSet['us_id'] = range(1, 1+len(cleanedSet))\n",
    "        action_id = []\n",
    "        for num in range(1, 1+len(cleanedSet)):\n",
    "             action_id.append(f\"action_{num}\")          \n",
    "        cleanedSet['action_id'] = action_id\n",
    "        benefit_id = []\n",
    "        for num in range(1, 1+len(cleanedSet)):\n",
    "             benefit_id.append(f\"benefit_{num}\")          \n",
    "        cleanedSet['benefit_id'] = benefit_id\n",
    "        \n",
    "        mypath = Path().absolute()\n",
    "        csvpath = mypath / 'USSetwithBenefit.csv'\n",
    "        cleanedSet.to_csv(csvpath, sep=';', header=True, index=False)\n",
    "        return csvpath\n",
    "        \n",
    "\n",
    "    def main(file, heuristic_number):\n",
    "        json_dict = json.loads((str(json.dumps(Converter.printStories(file)))))\n",
    "        actorsCol, mainVerbsCol, mainObjCol, benefitVerbCol, benefitObjCol = Converter.getFeatures(json_dict)\n",
    "        csvpath = Converter.create_csv(actorsCol, mainVerbsCol, mainObjCol, benefitVerbCol, benefitObjCol)\n",
    "        return QueryGraph.main(csvpath, heuristic_number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
